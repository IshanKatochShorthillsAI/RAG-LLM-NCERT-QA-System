{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57f19a6-1b6d-4130-9ad5-9fad78b03652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2021.10.8 (from selenium)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/shtlp_0133/.local/lib/python3.10/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/shtlp_0133/.local/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/shtlp_0133/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /home/shtlp_0133/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.8)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/shtlp_0133/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/shtlp_0133/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/shtlp_0133/.local/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, urllib3, pysocks, outcome, certifi, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed certifi-2025.1.31 outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.29.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.2 urllib3-2.3.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50608f49-fa9f-4707-baa1-43fce9ba4d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de19295-35a1-4b9f-a5d1-fadb0454387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Class 7...\n",
      "Found download link for Class 7: https://ncert.nic.in/textbook/pdf/gesc1dd.zip\n",
      "Processing Class 8...\n",
      "Found download link for Class 8: https://ncert.nic.in/textbook/pdf/hesc1dd.zip\n",
      "Processing Class 9...\n",
      "Found download link for Class 9: https://ncert.nic.in/textbook/pdf/iesc1dd.zip\n",
      "Processing Class 10...\n",
      "Found download link for Class 10: https://ncert.nic.in/textbook/pdf/jesc1dd.zip\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "\n",
    "# Initialize Firefox driver\n",
    "serv_obj = Service(\"/snap/bin/firefox.geckodriver\")  # Update this path as needed\n",
    "firefox_options = webdriver.FirefoxOptions()\n",
    "driver = webdriver.Firefox(service=serv_obj, options=firefox_options)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "# URL for the NCERT textbook page\n",
    "url = \"https://ncert.nic.in/textbook.php\"\n",
    "\n",
    "# Loop over classes 7 to 10\n",
    "for class_num in range(7, 11):\n",
    "    driver.get(url)\n",
    "    print(f\"Processing Class {class_num}...\")\n",
    "    \n",
    "    # Select class from the dropdown\n",
    "    class_dropdown = Select(driver.find_element(By.NAME, \"tclass\"))\n",
    "    class_dropdown.select_by_value(str(class_num))\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Select subject \"Science\" (adjust if necessary)\n",
    "    subject_dropdown = Select(driver.find_element(By.NAME, \"tsubject\"))\n",
    "    subject_dropdown.select_by_visible_text(\"Science\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Select book \"Science\" (adjust if necessary)\n",
    "    book_dropdown = Select(driver.find_element(By.NAME, \"tbook\"))\n",
    "    book_dropdown.select_by_visible_text(\"Science\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Click the \"Go\" button to navigate to the next page\n",
    "    go_button = driver.find_element(By.XPATH, \"//input[@value='Go']\")\n",
    "    go_button.click()\n",
    "    time.sleep(5)  # wait for the page to load\n",
    "    \n",
    "    # Wait for the sidebar container that holds the download links\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "    sidebar_container = wait.until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//td[@class='sidebar-menu']\"))\n",
    "    )\n",
    "    \n",
    "    # Find all <a> elements inside the sidebar container and pick the last one\n",
    "    links = sidebar_container.find_elements(By.TAG_NAME, \"a\")\n",
    "    if links:\n",
    "        download_link = links[-1]\n",
    "        download_url = download_link.get_attribute(\"href\")\n",
    "        print(f\"Found download link for Class {class_num}: {download_url}\")\n",
    "        download_link.click()\n",
    "        \n",
    "        # Increase waiting time to ensure the download starts/completes\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        print(f\"No download link found for Class {class_num}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e18b0b1-d638-4ef8-941d-6f6e4b043868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file_path = '/home/shtlp_0133/Downloads/jesc1dd.zip' \n",
    "extract_dir = '/home/shtlp_0133/Documents/RAG Assignment 17 March/Initial Dataset/Science Grade 10'  \n",
    "\n",
    "# Open the zip file and extract all its contents\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728cf10-7a12-4b2f-8ad7-c0c1f1310466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d8b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: venv/bin/activate: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd55e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in ./.venv/lib/python3.12/site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "asttokens               3.0.0\n",
      "comm                    0.2.2\n",
      "debugpy                 1.8.13\n",
      "decorator               5.2.1\n",
      "executing               2.2.0\n",
      "ipykernel               6.29.5\n",
      "ipython                 9.0.2\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.7.2\n",
      "matplotlib-inline       0.1.7\n",
      "nest-asyncio            1.6.0\n",
      "packaging               24.2\n",
      "parso                   0.8.4\n",
      "pexpect                 4.9.0\n",
      "pip                     25.0.1\n",
      "platformdirs            4.3.6\n",
      "prompt_toolkit          3.0.50\n",
      "psutil                  7.0.0\n",
      "ptyprocess              0.7.0\n",
      "pure_eval               0.2.3\n",
      "Pygments                2.19.1\n",
      "PyPDF2                  3.0.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pyzmq                   26.3.0\n",
      "six                     1.17.0\n",
      "stack-data              0.6.3\n",
      "tornado                 6.4.2\n",
      "traitlets               5.14.3\n",
      "wcwidth                 0.2.13\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1765014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Pass strict=False\n",
    "        reader = PdfReader(pdf_path, strict=False)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {pdf_path}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960b5279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: Initial Dataset/Science Grade 7/gesc105.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc112.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc111.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc103.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc106.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc108.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc102.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc104.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc113.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc110.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc109.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc107.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 7/gesc101.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc103.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc107.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc104.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc113.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc105.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc106.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc101.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc110.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc102.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc112.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc109.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc111.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 8/hesc108.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc104.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc112.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc109.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc103.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc106.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc102.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc111.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc105.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc108.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc107.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc110.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 9/iesc101.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc102.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc113.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc103.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc112.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc106.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc109.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc104.pdf\n",
      "Failed to read Initial Dataset/Science Grade 10/jesc104.pdf: Invalid Elementary Object starting with b'\\x0c' @1740309: b'\\r\\x00DI\\r\\x00DH\\r\\x00CG\\r\\x00BF\\r\\x00AE\\x0c\\x00AE\\x0c\\x00@D\\x0c\\x00?C\\x0c\\x00?B\\x0c\\x00?B\\x0c\\x00>A\\x0c\\x00=@\\x0c\\x00<>\\x0c\\x00<>\\x0c\\x00<>\\x0c\\x00;<\\x0c\\x00;<\\x0c\\x00;;\\x0c\\x00:;\\x0c\\x00:;'\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc111.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc101.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc110.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc107.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc108.pdf\n",
      "Extracting text from: Initial Dataset/Science Grade 10/jesc105.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"Initial Dataset\"  # Path to the \"Initial Dataset\" folder\n",
    "\n",
    "# Dictionary to hold all extracted text, e.g. { \"Science Grade 7\": {\"gesc101.pdf\": \"Text...\", ...}, ... }\n",
    "grade_texts = {}\n",
    "\n",
    "# List the folders (grades) you want to process\n",
    "grades = [\"Science Grade 7\", \"Science Grade 8\", \"Science Grade 9\", \"Science Grade 10\"]\n",
    "\n",
    "for grade_folder in grades:\n",
    "    folder_path = os.path.join(base_dir, grade_folder)\n",
    "    \n",
    "    # A sub-dictionary to hold {pdf_filename: extracted_text} for this grade\n",
    "    grade_texts[grade_folder] = {}\n",
    "    \n",
    "    # List all PDF files in the grade folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, file_name)\n",
    "            print(f\"Extracting text from: {pdf_path}\")\n",
    "            \n",
    "            # Extract text\n",
    "            pdf_text = extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            # Store in dictionary\n",
    "            grade_texts[grade_folder][file_name] = pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa2b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grades processed: ['Science Grade 7', 'Science Grade 8', 'Science Grade 9', 'Science Grade 10']\n",
      "\n",
      "PDF: gesc105.pdf\n",
      "5\n",
      "Every day you come across many\n",
      "changes in your surroundings.\n",
      "These changes may involve one\n",
      "or more substances.  For example, your\n",
      "mother may ask you to dissolve sugar\n",
      "in water to make a cold drink. Making a\n",
      "sugar solution is a change. Similarly,setting curd from milk is a change.\n",
      "Sometimes milk be\n"
     ]
    }
   ],
   "source": [
    "# Print all grade keys\n",
    "print(\"Grades processed:\", list(grade_texts.keys()))\n",
    "\n",
    "# For a particular grade (say \"Science Grade 7\"), print a few PDF filenames and the first 300 characters of their text.\n",
    "grade = \"Science Grade 7\"\n",
    "for pdf_file, text in grade_texts.get(grade, {}).items():\n",
    "    print(f\"\\nPDF: {pdf_file}\")\n",
    "    print(text[:300])  # prints the first 300 characters\n",
    "    # Break after viewing one or two samples, if desired:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4736f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# After processing:\n",
    "with open(\"grade_texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grade_texts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989d932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "639891ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence_transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Using cached scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached setuptools-76.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-76.0.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (145 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, setuptools, safetensors, regex, pyyaml, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, fsspec, filelock, charset-normalizer, certifi, scipy, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, scikit-learn, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.17.0 fsspec-2025.3.0 huggingface-hub-0.29.3 idna-3.10 jinja2-3.1.6 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence_transformers-3.4.1 setuptools-76.0.0 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.6.0 tqdm-4.67.1 transformers-4.49.0 triton-3.2.0 typing-extensions-4.12.2 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6097b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shtlp_0133/Documents/RAG Assignment 17 March/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the embedding model (using a popular SentenceTransformer model)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84a4c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grades processed: ['Science Grade 7', 'Science Grade 8', 'Science Grade 9', 'Science Grade 10']\n",
      "\n",
      "Grade: Science Grade 7 - Sample PDF: gesc105.pdf\n",
      "5\n",
      "Every day you come across many\n",
      "changes in your surroundings.\n",
      "These changes may involve one\n",
      "or more substances.  For example, your\n",
      "mother may ask you to dissolve sugar\n",
      "in water to make a cold drink.  ...\n",
      "\n",
      "Grade: Science Grade 8 - Sample PDF: hesc103.pdf\n",
      "COAL AND PETROLEUM COAL AND PETROLEUM\n",
      "Can air , water and soil be exhausted\n",
      "by human activities? Y ou have alr eady\n",
      "studied about water in Class VII. Is water\n",
      "a limitless resource?\n",
      "In the light of the ...\n",
      "\n",
      "Grade: Science Grade 9 - Sample PDF: iesc104.pdf\n",
      "In Chapter 3, we have lear nt that atoms and\n",
      "molecules ar e the fundamental building\n",
      "blocks of matter . The existence of dif ferent\n",
      "kinds of matter is due to dif ferent atoms\n",
      "constituting them. Now th ...\n",
      "\n",
      "Grade: Science Grade 10 - Sample PDF: jesc102.pdf\n",
      "Acids, Bases\n",
      "and Salts2 CHAPTER\n",
      "You have lear nt in your pr evious classes that the sour and bitter\n",
      "tastes of food are due to acids and bases, respectively, present in them.\n",
      "If someone in the family i ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Define the path to your JSON file\n",
    "json_file_path = \"grade_texts.json\"\n",
    "\n",
    "# Load the JSON file into a Python dictionary\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    grade_texts = json.load(f)\n",
    "\n",
    "# Optional: Print a summary of the loaded data\n",
    "print(\"Grades processed:\", list(grade_texts.keys()))\n",
    "for grade in grade_texts:\n",
    "    # Show one PDF filename and a snippet of its text\n",
    "    sample_pdf = list(grade_texts[grade].keys())[0]\n",
    "    sample_text = grade_texts[grade][sample_pdf]\n",
    "    print(f\"\\nGrade: {grade} - Sample PDF: {sample_pdf}\")\n",
    "    print(sample_text[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8abf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Splits the input text into smaller chunks.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): The text to be chunked.\n",
    "      chunk_size (int): Desired number of characters per chunk.\n",
    "      overlap (int): Overlap between consecutive chunks.\n",
    "      \n",
    "    Returns:\n",
    "      List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start += chunk_size - overlap  # slide window with overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57d822",
   "metadata": {},
   "source": [
    "# Basic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8b0f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 41 chunks for gesc105.pdf in Science Grade 7\n",
      "Processed 46 chunks for gesc112.pdf in Science Grade 7\n",
      "Processed 58 chunks for gesc111.pdf in Science Grade 7\n",
      "Processed 53 chunks for gesc103.pdf in Science Grade 7\n",
      "Processed 47 chunks for gesc106.pdf in Science Grade 7\n",
      "Processed 37 chunks for gesc108.pdf in Science Grade 7\n",
      "Processed 49 chunks for gesc102.pdf in Science Grade 7\n",
      "Processed 34 chunks for gesc104.pdf in Science Grade 7\n",
      "Processed 41 chunks for gesc113.pdf in Science Grade 7\n",
      "Processed 51 chunks for gesc110.pdf in Science Grade 7\n",
      "Processed 62 chunks for gesc109.pdf in Science Grade 7\n",
      "Processed 45 chunks for gesc107.pdf in Science Grade 7\n",
      "Processed 41 chunks for gesc101.pdf in Science Grade 7\n",
      "Processed 28 chunks for hesc103.pdf in Science Grade 8\n",
      "Processed 58 chunks for hesc107.pdf in Science Grade 8\n",
      "Processed 52 chunks for hesc104.pdf in Science Grade 8\n",
      "Processed 65 chunks for hesc113.pdf in Science Grade 8\n",
      "Processed 51 chunks for hesc105.pdf in Science Grade 8\n",
      "Processed 49 chunks for hesc106.pdf in Science Grade 8\n",
      "Processed 61 chunks for hesc101.pdf in Science Grade 8\n",
      "Processed 53 chunks for hesc110.pdf in Science Grade 8\n",
      "Processed 56 chunks for hesc102.pdf in Science Grade 8\n",
      "Processed 55 chunks for hesc112.pdf in Science Grade 8\n",
      "Processed 40 chunks for hesc109.pdf in Science Grade 8\n",
      "Processed 49 chunks for hesc111.pdf in Science Grade 8\n",
      "Processed 74 chunks for hesc108.pdf in Science Grade 8\n",
      "Processed 55 chunks for iesc104.pdf in Science Grade 9\n",
      "Processed 78 chunks for iesc112.pdf in Science Grade 9\n",
      "Processed 73 chunks for iesc109.pdf in Science Grade 9\n",
      "Processed 57 chunks for iesc103.pdf in Science Grade 9\n",
      "Processed 57 chunks for iesc106.pdf in Science Grade 9\n",
      "Processed 55 chunks for iesc102.pdf in Science Grade 9\n",
      "Processed 73 chunks for iesc111.pdf in Science Grade 9\n",
      "Processed 72 chunks for iesc105.pdf in Science Grade 9\n",
      "Processed 76 chunks for iesc108.pdf in Science Grade 9\n",
      "Processed 79 chunks for iesc107.pdf in Science Grade 9\n",
      "Processed 77 chunks for iesc110.pdf in Science Grade 9\n",
      "Processed 67 chunks for iesc101.pdf in Science Grade 9\n",
      "Processed 88 chunks for jesc102.pdf in Science Grade 10\n",
      "Processed 46 chunks for jesc113.pdf in Science Grade 10\n",
      "Processed 97 chunks for jesc103.pdf in Science Grade 10\n",
      "Processed 68 chunks for jesc112.pdf in Science Grade 10\n",
      "Processed 69 chunks for jesc106.pdf in Science Grade 10\n",
      "Processed 130 chunks for jesc109.pdf in Science Grade 10\n",
      "Processed 29 chunks for jesc104.pdf in Science Grade 10\n",
      "Processed 102 chunks for jesc111.pdf in Science Grade 10\n",
      "Processed 71 chunks for jesc101.pdf in Science Grade 10\n",
      "Processed 57 chunks for jesc110.pdf in Science Grade 10\n",
      "Processed 85 chunks for jesc107.pdf in Science Grade 10\n",
      "Processed 36 chunks for jesc108.pdf in Science Grade 10\n",
      "Processed 118 chunks for jesc105.pdf in Science Grade 10\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold embeddings for each grade.\n",
    "# Structure: { grade: [ { 'pdf': pdf_filename, 'chunk_index': idx, 'text': chunk, 'embedding': vector }, ... ] }\n",
    "embeddings_by_grade = {}\n",
    "\n",
    "for grade, pdf_dict in grade_texts.items():\n",
    "    embeddings_by_grade[grade] = []  # initialize list for current grade\n",
    "    for pdf_filename, text in pdf_dict.items():\n",
    "        # Chunk the text for this PDF\n",
    "        chunks = chunk_text(text, chunk_size=500, overlap=50)\n",
    "        # Compute embeddings for all chunks in a batch for efficiency\n",
    "        chunk_embeddings = model.encode(chunks)\n",
    "        \n",
    "        # Save each chunk's embedding along with metadata\n",
    "        for idx, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "            embeddings_by_grade[grade].append({\n",
    "                'pdf': pdf_filename,\n",
    "                'chunk_index': idx,\n",
    "                'text': chunk,\n",
    "                'embedding': embedding.tolist()  # converting numpy array to list for JSON compatibility if needed\n",
    "            })\n",
    "        \n",
    "        print(f\"Processed {len(chunks)} chunks for {pdf_filename} in {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74629027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embeddings for Science Grade 7:\n",
      "\n",
      "PDF: gesc105.pdf, Chunk: 0\n",
      "Text snippet: 5\n",
      "Every day you come across many\n",
      "changes in your surroundings.\n",
      "These changes may involve one\n",
      "or more ...\n",
      "Embedding shape: (384,)\n",
      "--------------------------------------------------\n",
      "PDF: gesc105.pdf, Chunk: 1\n",
      "Text snippet: is chapter we shall perform some\n",
      "activities and study the nature of thesechanges.  Broadly, these ch ...\n",
      "Embedding shape: (384,)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose a grade to inspect\n",
    "grade_to_inspect = \"Science Grade 7\"\n",
    "\n",
    "print(f\"Sample embeddings for {grade_to_inspect}:\\n\")\n",
    "# Show the first two chunks for inspection\n",
    "for item in embeddings_by_grade[grade_to_inspect][:2]:\n",
    "    print(f\"PDF: {item['pdf']}, Chunk: {item['chunk_index']}\")\n",
    "    print(\"Text snippet:\", item['text'][:100], \"...\")\n",
    "    print(\"Embedding shape:\", np.array(item['embedding']).shape)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f586ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4986f5c",
   "metadata": {},
   "source": [
    "# GenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e1c9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.164.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.30.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic (from google-generativeai)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in ./.venv/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->google-generativeai)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Using cached google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading google_api_python_client-2.164.0-py2.py3-none-any.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl (293 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pydantic-core, pyasn1, protobuf, grpcio, cachetools, annotated-types, rsa, pydantic, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.164.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 googleapis-common-protos-1.69.1 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.6 pydantic-core-2.27.2 pyparsing-3.2.1 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6556229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a1f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini embedding function is ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shtlp_0133/Documents/RAG Assignment 17 March/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API using the environment variable GEMINI_API_KEY\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not gemini_api_key:\n",
    "    raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "model_name = \"models/embedding-001\"\n",
    "\n",
    "def get_embedding(input_text):\n",
    "    \"\"\"\n",
    "    Computes the embedding for the given text using Gemini.\n",
    "    \"\"\"\n",
    "    title = \"Custom query\"\n",
    "    response = genai.embed_content(\n",
    "        model=model_name,\n",
    "        content=input_text,\n",
    "        task_type=\"retrieval_document\",\n",
    "        title=title\n",
    "    )\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "print(\"Gemini embedding function is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28169461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 46 chunks for gesc105.pdf in Science Grade 7\n",
      "Processed 52 chunks for gesc112.pdf in Science Grade 7\n",
      "Processed 65 chunks for gesc111.pdf in Science Grade 7\n",
      "Processed 59 chunks for gesc103.pdf in Science Grade 7\n",
      "Processed 52 chunks for gesc106.pdf in Science Grade 7\n",
      "Processed 42 chunks for gesc108.pdf in Science Grade 7\n",
      "Processed 55 chunks for gesc102.pdf in Science Grade 7\n",
      "Processed 38 chunks for gesc104.pdf in Science Grade 7\n",
      "Skipping empty chunk 46 from gesc113.pdf\n",
      "Processed 47 chunks for gesc113.pdf in Science Grade 7\n",
      "Processed 57 chunks for gesc110.pdf in Science Grade 7\n",
      "Processed 70 chunks for gesc109.pdf in Science Grade 7\n",
      "Processed 51 chunks for gesc107.pdf in Science Grade 7\n",
      "Processed 46 chunks for gesc101.pdf in Science Grade 7\n",
      "Processed 32 chunks for hesc103.pdf in Science Grade 8\n",
      "Processed 65 chunks for hesc107.pdf in Science Grade 8\n",
      "Processed 59 chunks for hesc104.pdf in Science Grade 8\n",
      "Processed 73 chunks for hesc113.pdf in Science Grade 8\n",
      "Processed 57 chunks for hesc105.pdf in Science Grade 8\n",
      "Processed 55 chunks for hesc106.pdf in Science Grade 8\n",
      "Processed 69 chunks for hesc101.pdf in Science Grade 8\n",
      "Processed 59 chunks for hesc110.pdf in Science Grade 8\n",
      "Processed 63 chunks for hesc102.pdf in Science Grade 8\n",
      "Processed 62 chunks for hesc112.pdf in Science Grade 8\n",
      "Processed 45 chunks for hesc109.pdf in Science Grade 8\n",
      "Processed 56 chunks for hesc111.pdf in Science Grade 8\n",
      "Skipping empty chunk 83 from hesc108.pdf\n",
      "Processed 84 chunks for hesc108.pdf in Science Grade 8\n",
      "Processed 62 chunks for iesc104.pdf in Science Grade 9\n",
      "Processed 88 chunks for iesc112.pdf in Science Grade 9\n",
      "Processed 82 chunks for iesc109.pdf in Science Grade 9\n",
      "Processed 64 chunks for iesc103.pdf in Science Grade 9\n",
      "Processed 64 chunks for iesc106.pdf in Science Grade 9\n",
      "Processed 62 chunks for iesc102.pdf in Science Grade 9\n",
      "Processed 82 chunks for iesc111.pdf in Science Grade 9\n",
      "Processed 81 chunks for iesc105.pdf in Science Grade 9\n",
      "Processed 85 chunks for iesc108.pdf in Science Grade 9\n",
      "Processed 89 chunks for iesc107.pdf in Science Grade 9\n",
      "Processed 86 chunks for iesc110.pdf in Science Grade 9\n",
      "Processed 75 chunks for iesc101.pdf in Science Grade 9\n",
      "Processed 99 chunks for jesc102.pdf in Science Grade 10\n",
      "Processed 52 chunks for jesc113.pdf in Science Grade 10\n",
      "Skipping empty chunk 108 from jesc103.pdf\n",
      "Processed 109 chunks for jesc103.pdf in Science Grade 10\n",
      "Processed 77 chunks for jesc112.pdf in Science Grade 10\n",
      "Processed 77 chunks for jesc106.pdf in Science Grade 10\n",
      "Processed 146 chunks for jesc109.pdf in Science Grade 10\n",
      "Processed 32 chunks for jesc104.pdf in Science Grade 10\n",
      "Processed 114 chunks for jesc111.pdf in Science Grade 10\n",
      "Processed 80 chunks for jesc101.pdf in Science Grade 10\n",
      "Processed 65 chunks for jesc110.pdf in Science Grade 10\n",
      "Processed 96 chunks for jesc107.pdf in Science Grade 10\n",
      "Processed 40 chunks for jesc108.pdf in Science Grade 10\n",
      "Processed 133 chunks for jesc105.pdf in Science Grade 10\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold embeddings for each grade.\n",
    "# Structure: { grade: [ { 'pdf': pdf_filename, 'chunk_index': idx, 'text': chunk, 'embedding': vector }, ... ] }\n",
    "embeddings_by_grade = {}\n",
    "\n",
    "for grade, pdf_dict in grade_texts.items():\n",
    "    embeddings_by_grade[grade] = []\n",
    "    for pdf_filename, text in pdf_dict.items():\n",
    "        # Chunk the text\n",
    "        chunks = chunk_text(text, chunk_size=500, overlap=100)\n",
    "        \n",
    "        # Compute embeddings for each chunk using Gemini\n",
    "        chunk_embeddings = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            if not chunk.strip():\n",
    "                print(f\"Skipping empty chunk {idx} from {pdf_filename}\")\n",
    "                continue  # Skip empty chunk\n",
    "            embedding = get_embedding(chunk)\n",
    "            chunk_embeddings.append(embedding)\n",
    "\n",
    "        \n",
    "        # Store embeddings and metadata\n",
    "        for idx, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "            embeddings_by_grade[grade].append({\n",
    "                'pdf': pdf_filename,\n",
    "                'chunk_index': idx,\n",
    "                'text': chunk,\n",
    "                'embedding': embedding  # already a list\n",
    "            })\n",
    "        print(f\"Processed {len(chunks)} chunks for {pdf_filename} in {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a43e680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample embeddings for Science Grade 7:\n",
      "\n",
      "PDF: gesc105.pdf, Chunk: 0\n",
      "Text snippet: 5\n",
      "Every day you come across many\n",
      "changes in your surroundings.\n",
      "These changes may involve one\n",
      "or more ...\n",
      "Embedding shape: (768,)\n",
      "--------------------------------------------------\n",
      "PDF: gesc105.pdf, Chunk: 1\n",
      "Text snippet: of ten changes you have\n",
      "noticed around you.\n",
      "In this chapter we shall perform some\n",
      "activities and stu ...\n",
      "Embedding shape: (768,)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inspect sample embeddings for a specific grade\n",
    "import numpy as np\n",
    "\n",
    "grade_to_inspect = \"Science Grade 7\"\n",
    "print(f\"\\nSample embeddings for {grade_to_inspect}:\\n\")\n",
    "for item in embeddings_by_grade[grade_to_inspect][:2]:\n",
    "    print(f\"PDF: {item['pdf']}, Chunk: {item['chunk_index']}\")\n",
    "    print(\"Text snippet:\", item['text'][:100], \"...\")\n",
    "    print(\"Embedding shape:\", np.array(item['embedding']).shape)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be0d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3cd8c2",
   "metadata": {},
   "source": [
    "# Vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e364073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weaviate-client\n",
      "  Downloading weaviate_client-4.11.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting httpx<0.29.0,>=0.26.0 (from weaviate-client)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting validators==0.34.0 (from weaviate-client)\n",
      "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting authlib<1.3.2,>=1.2.1 (from weaviate-client)\n",
      "  Downloading Authlib-1.3.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in ./.venv/lib/python3.12/site-packages (from weaviate-client) (2.10.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in ./.venv/lib/python3.12/site-packages (from weaviate-client) (1.71.0)\n",
      "Collecting grpcio-tools<2.0.0,>=1.66.2 (from weaviate-client)\n",
      "  Downloading grpcio_tools-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.66.2 (from weaviate-client)\n",
      "  Downloading grpcio_health_checking-1.71.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting cryptography (from authlib<1.3.2,>=1.2.1->weaviate-client)\n",
      "  Using cached cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in ./.venv/lib/python3.12/site-packages (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client) (5.29.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client) (76.0.0)\n",
      "Collecting anyio (from httpx<0.29.0,>=0.26.0->weaviate-client)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<0.29.0,>=0.26.0->weaviate-client)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.10)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (4.12.2)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.12 (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading weaviate_client-4.11.1-py3-none-any.whl (353 kB)\n",
      "Downloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
      "Downloading Authlib-1.3.1-py2.py3-none-any.whl (223 kB)\n",
      "Downloading grpcio_health_checking-1.71.0-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio_tools-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m748.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: validators, sniffio, pycparser, h11, grpcio-tools, grpcio-health-checking, httpcore, cffi, anyio, httpx, cryptography, authlib, weaviate-client\n",
      "Successfully installed anyio-4.8.0 authlib-1.3.1 cffi-1.17.1 cryptography-44.0.2 grpcio-health-checking-1.71.0 grpcio-tools-1.71.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 pycparser-2.22 sniffio-1.3.1 validators-0.34.0 weaviate-client-4.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4aae65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud/v1/.well-known/ready \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Weaviate: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Make sure you have your environment variables set:\n",
    "# WEAVIATE_URL, e.g., \"https://your-cluster-url.asia-southeast1.gcp.weaviate.cloud\"\n",
    "# WEAVIATE_API_KEY, your API key string\n",
    "\n",
    "WEAVIATE_URL=\"https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
    "WEAVIATE_API_KEY=\"kM3zyRvp1U7RsixnCtDlYy2Ex8QHt1kcNAfS\"\n",
    "\n",
    "# Connect to Weaviate Cloud\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_API_KEY)\n",
    ")\n",
    "\n",
    "print(\"Connected to Weaviate:\", client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf02a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes as wvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10e8707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing DocumentChunks collection.\n",
      "Collection 'DocumentChunks' created successfully!\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "# (Optional) Delete an existing collection named \"DocumentChunks\"\n",
    "try:\n",
    "    client.collections.delete(\"DocumentChunks\")\n",
    "    print(\"Deleted existing DocumentChunks collection.\")\n",
    "except Exception as e:\n",
    "    print(\"No existing collection to delete or deletion error:\", e)\n",
    "\n",
    "# Create the collection by passing parameters directly\n",
    "client.collections.create(\n",
    "    name=\"DocumentChunks\",\n",
    "    description=\"Text chunks extracted from NCERT PDFs with metadata and Gemini embeddings.\",\n",
    "    properties=[\n",
    "        wvc.config.Property(\n",
    "            name=\"text\",\n",
    "            data_type=wvc.config.DataType.TEXT,\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"grade\",\n",
    "            data_type=wvc.config.DataType.TEXT,\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"source\",\n",
    "            data_type=wvc.config.DataType.TEXT,\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"chunk_index\",\n",
    "            data_type=wvc.config.DataType.INT,\n",
    "        )\n",
    "    ],\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.none()\n",
    ")\n",
    "print(\"Collection 'DocumentChunks' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51b274b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted into Weaviate successfully!\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the collection object we just created\n",
    "collection_obj = client.collections.get(\"DocumentChunks\")\n",
    "\n",
    "# Insert data using dynamic batch insertion\n",
    "with collection_obj.batch.dynamic() as batch:\n",
    "    for grade, chunks in embeddings_by_grade.items():\n",
    "        for chunk in chunks:\n",
    "            properties = {\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"grade\": grade,\n",
    "                \"source\": chunk[\"pdf\"],\n",
    "                \"chunk_index\": chunk[\"chunk_index\"]\n",
    "            }\n",
    "            vector = chunk[\"embedding\"]  # Pre-computed Gemini embedding (a list)\n",
    "            batch.add_object(properties=properties, vector=vector)\n",
    "\n",
    "print(\"Documents inserted into Weaviate successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a085c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "/home/shtlp_0133/Documents/RAG Assignment 17 March/.venv/lib/python3.12/site-packages/weaviate/warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n",
      "INFO:httpx:HTTP Request: DELETE https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud/v1/schema/DocumentChunks_All \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: DocumentChunks_All\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud/v1/schema \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud/v1/schema/DocumentChunks_All \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'DocumentChunks_All' created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742195829.196617   36073 chttp2_transport.cc:1201] ipv4:34.160.174.119:443: Got goaway [1] err=UNAVAILABLE:GOAWAY received; Error code: 1; Debug Text: wrong_frame_sequence {created_time:\"2025-03-17T12:47:09.19660075+05:30\", http2_error:1, grpc_status:14}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted into collection 'DocumentChunks_All' successfully!\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "# Use a unified collection name\n",
    "COLLECTION_NAME = \"DocumentChunks_All\"\n",
    "\n",
    "# Initialize the Weaviate client\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.getenv(\"WEAVIATE_URL\", \"https://oxhv5pqtk6xl62xb6gq.c0.asia-southeast1.gcp.weaviate.cloud\"),\n",
    "    auth_credentials=wvc.init.Auth.api_key(os.getenv(\"WEAVIATE_API_KEY\"))\n",
    ")\n",
    "\n",
    "# Optional: Delete the unified collection if it already exists\n",
    "try:\n",
    "    client.collections.delete(COLLECTION_NAME)\n",
    "    print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing collection '{COLLECTION_NAME}' to delete, or deletion error: {e}\")\n",
    "\n",
    "# Create the unified collection\n",
    "try:\n",
    "    client.collections.create(\n",
    "        name=COLLECTION_NAME,\n",
    "        description=\"Text chunks extracted from NCERT PDFs with metadata and Gemini embeddings from all grades.\",\n",
    "        properties=[\n",
    "            wvc.config.Property(\n",
    "                name=\"text\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"grade\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"source\",\n",
    "                data_type=wvc.config.DataType.TEXT,\n",
    "            ),\n",
    "            wvc.config.Property(\n",
    "                name=\"chunk_index\",\n",
    "                data_type=wvc.config.DataType.INT,\n",
    "            )\n",
    "        ],\n",
    "        vectorizer_config=wvc.config.Configure.Vectorizer.none()\n",
    "    )\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating collection {COLLECTION_NAME}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Retrieve the collection object\n",
    "collection_obj = client.collections.get(COLLECTION_NAME)\n",
    "\n",
    "# Assume embeddings_by_grade is a dictionary where each key is a grade and each value is a list of document chunks.\n",
    "# Each chunk is a dictionary with keys: \"text\", \"pdf\", \"chunk_index\", and \"embedding\".\n",
    "# For example:\n",
    "# embeddings_by_grade = {\n",
    "#     \"Science Grade 7\": [ { \"text\": \"sample text\", \"pdf\": \"gesc105.pdf\", \"chunk_index\": 0, \"embedding\": [...] }, ... ],\n",
    "#     \"Science Grade 8\": [ ... ],\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Use dynamic batch insertion to insert all documents into the unified collection\n",
    "try:\n",
    "    with collection_obj.batch.dynamic() as batch:\n",
    "        for grade, chunks in embeddings_by_grade.items():\n",
    "            for chunk in chunks:\n",
    "                properties = {\n",
    "                    \"text\": chunk[\"text\"],        # e.g., your \"text snippet\"\n",
    "                    \"grade\": grade,\n",
    "                    \"source\": chunk[\"pdf\"],\n",
    "                    \"chunk_index\": chunk[\"chunk_index\"]\n",
    "                }\n",
    "                vector = chunk[\"embedding\"]       # Pre-computed Gemini embedding (list of floats)\n",
    "                batch.add_object(properties=properties, vector=vector)\n",
    "    print(f\"Documents inserted into collection '{COLLECTION_NAME}' successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data into {COLLECTION_NAME}: {e}\")\n",
    "\n",
    "# Optionally, close the client connection if needed\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d00acf",
   "metadata": {},
   "source": [
    "## Querying Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c6df32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Results for DocumentChunks_All\n",
      "Grade: Science Grade 10, Source: jesc105.pdf, Chunk: 30\n",
      "Text snippet: ctivity 5.2Activity 5.2\n",
      "Based on the two activities performed above, can we design an\n",
      "experiment to demonstrate that sunlight is essential for photosy ...\n",
      "Distance: 0.27250397205352783\n",
      "--------------------------------------------------\n",
      "Grade: Science Grade 10, Source: jesc105.pdf, Chunk: 18\n",
      "Text snippet: otrophs.\n",
      "Heterotrophic organisms include animals and fungi.\n",
      "5.2.1 Autotrophic Nutrition\n",
      "Carbon and energy requirements of the autotrophic organism are ...\n",
      "Distance: 0.2938883304595947\n",
      "--------------------------------------------------\n",
      "Grade: Science Grade 10, Source: jesc105.pdf, Chunk: 24\n",
      "Text snippet: xygen.\n",
      "(iii)Reduction of carbon dioxide to\n",
      "carbohydrates.\n",
      "These steps need not take place one after\n",
      "the other immediately. For example, desert\n",
      "plants  ...\n",
      "Distance: 0.29839253425598145\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "# Define your sample query and compute its embedding using your Gemini function\n",
    "query = \"What is photosynthesis?\"\n",
    "query_embedding = get_embedding(query)  # returns a list of floats\n",
    "\n",
    "# Choose the collection for a specific grade\n",
    "collection_name = \"DocumentChunks_All\"\n",
    "collection_obj = client.collections.get(collection_name)\n",
    "\n",
    "# Perform a near-vector search by passing parameters directly to near_vector()\n",
    "response = collection_obj.query.near_vector(\n",
    "    query_embedding, \n",
    "    limit=3, \n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")\n",
    "\n",
    "# Process and display the results\n",
    "print(\"\\nQuery Results for\", collection_name)\n",
    "for obj in response.objects:\n",
    "    props = obj.properties\n",
    "    print(f\"Grade: {props['grade']}, Source: {props['source']}, Chunk: {props['chunk_index']}\")\n",
    "    print(\"Text snippet:\", props[\"text\"][:150], \"...\")\n",
    "    print(\"Distance:\", obj.metadata.distance)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afd48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4190c6ce",
   "metadata": {},
   "source": [
    "# LLM Intergration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37f00ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shtlp_0133/Documents/RAG Assignment 17 March/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shtlp_0133/Documents/RAG Assignment 17 March/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/shtlp_0133/Documents/RAG Assignment 17 March/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\", use_auth_token=\"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\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37851031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for LLM:\n",
      "\n",
      "Answer the following question based solely on the provided context. Do not generate additional questions or answers. \n",
      "\n",
      "Context:\n",
      "n the\n",
      "presence of sunlight, it is called\n",
      "photosynthesis (Photo: light; synthesis  :\n",
      "to combine). So we find that chlorophyll,\n",
      "sunlight, carbon dioxide and water are\n",
      "necessary to carry out the process ofphotosynthesis. It is a unique process\n",
      "on the earth . The solar energy is\n",
      "captured by the leaves and stored in the\n",
      "plant in the form of food. Thus, sun is\n",
      "the ultimate source of energy for allliving organisms.\n",
      "Can you imagine life on earth in the\n",
      "absence of photosynthesis!\n",
      "In the absence of photos\n",
      "\n",
      "e carbon dioxide, water and\n",
      "minerals for the synthesis of food.\n",
      "Chlorophyll, water, carbon dioxide and sunlight are the essentialrequirements for photosynthesis.\n",
      "Complex chemical substances such as carbohydrates are the productsof photosynthesis.\n",
      "Solar energy is absorbed by the chlorophylls present in leaves/plants.\n",
      "Oxygen is produced during photosynthesis.\n",
      "Oxygen released in photosynthesis is utilised by living organisms fortheir survival.\n",
      "Many fungi derive nutrition from dead and decayin\n",
      "\n",
      "regulate the amount of light, water andcarbon dioxide to grow the plants.\n",
      "3. Try growing a sweet potato just in water. Describe your experiment and\n",
      "observations.You can read more on the following website:www.phschool.com/science/biology_place/biocoach/photosynth/overview.htmFig. 1.9  Experiment to test the\n",
      "occurrence of photosynthesis\n",
      "Did you know?\n",
      "Light is so important to plants that their leaves grow in many patterns soas to absorb maximum sunlight.\n",
      "Reprint 2024-25\n",
      "\n",
      "Question: What is photosynthesis?\n",
      "Answer:\n",
      "\n",
      "Generated Answer:\n",
      "Answer the following question based solely on the provided context. Do not generate additional questions or answers. \n",
      "\n",
      "Context:\n",
      "n the\n",
      "presence of sunlight, it is called\n",
      "photosynthesis (Photo: light; synthesis  :\n",
      "to combine). So we find that chlorophyll,\n",
      "sunlight, carbon dioxide and water are\n",
      "necessary to carry out the process ofphotosynthesis. It is a unique process\n",
      "on the earth. The solar energy is\n",
      "captured by the leaves and stored in the\n",
      "plant in the form of food. Thus, sun is\n",
      "the ultimate source of energy for allliving organisms.\n",
      "Can you imagine life on earth in the\n",
      "absence of photosynthesis!\n",
      "In the absence of photos\n",
      "\n",
      "e carbon dioxide, water and\n",
      "minerals for the synthesis of food.\n",
      "Chlorophyll, water, carbon dioxide and sunlight are the essentialrequirements for photosynthesis.\n",
      "Complex chemical substances such as carbohydrates are the productsof photosynthesis.\n",
      "Solar energy is absorbed by the chlorophylls present in leaves/plants.\n",
      "Oxygen is produced during photosynthesis.\n",
      "Oxygen released in photosynthesis is utilised by living organisms fortheir survival.\n",
      "Many fungi derive nutrition from dead and decayin\n",
      "\n",
      "regulate the amount of light, water andcarbon dioxide to grow the plants.\n",
      "3. Try growing a sweet potato just in water. Describe your experiment and\n",
      "observations.You can read more on the following website:www.phschool.com/science/biology_place/biocoach/photosynth/overview.htmFig. 1.9  Experiment to test the\n",
      "occurrence of photosynthesis\n",
      "Did you know?\n",
      "Light is so important to plants that their leaves grow in many patterns soas to absorb maximum sunlight.\n",
      "Reprint 2024-25\n",
      "\n",
      "Question: What is photosynthesis?\n",
      "Answer: Photosynthesis is the process by which plants and other organisms convert sunlight into chemical energy. This energy is stored in the form of carbohydrates, which are used by the organism for growth and development. Photosynthesis is an important process in the carbon cycle, which is the movement of carbon between the atmosphere, the oceans, and the land. The carbon cycle is essential for the survival of all living organisms on Earth.\n"
     ]
    }
   ],
   "source": [
    "# Use the 'response' variable from your Weaviate query cell.\n",
    "# Combine the retrieved chunks' text into a single context string.\n",
    "context = \"\\n\\n\".join([obj.properties[\"text\"] for obj in response.objects])\n",
    "query = \"What is photosynthesis?\"\n",
    "\n",
    "# Construct a stricter prompt that instructs the model to answer only the given question.\n",
    "prompt = (\n",
    "    \"Answer the following question based solely on the provided context. \"\n",
    "    \"Do not generate additional questions or answers. \"\n",
    "    \"\\n\\nContext:\\n\" + context +\n",
    "    \"\\n\\nQuestion: \" + query +\n",
    "    \"\\nAnswer:\"\n",
    ")\n",
    "\n",
    "print(\"Prompt for LLM:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate answer with tuned parameters for more focused output.\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=512,      # Lower max length for a concise answer\n",
    "    do_sample=True,      # Enable sampling\n",
    "    temperature=0.3,     # Lower temperature for less randomness\n",
    "    top_p=0.8,           # Lower top_p for more conservative sampling\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id  # Stop generation at EOS token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b545f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f10e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
